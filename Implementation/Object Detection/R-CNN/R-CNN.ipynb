{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"R-CNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMenAstGpUpVsMow/OlTLCL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QNKwCS6MFdt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620308788376,"user_tz":-540,"elapsed":24560,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"387b7f88-c65d-422b-8e78-5ded83a4e94b"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C06VKAAIF_jN","executionInfo":{"status":"ok","timestamp":1620308788376,"user_tz":-540,"elapsed":5384,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"247ee081-4ceb-47a9-94a0-73a2abb6ab74"},"source":["%cd '/content/gdrive/MyDrive/PASCAL/VOCdevkit/VOC2007/'\n","%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/PASCAL/VOCdevkit/VOC2007\n","\u001b[0m\u001b[01;34mAnnotations\u001b[0m/  \u001b[01;34mImageSets\u001b[0m/   seg_df.csv          \u001b[01;34mSegmentationObject\u001b[0m/\n","df.csv        \u001b[01;34mJPEGImages\u001b[0m/  \u001b[01;34mSegmentationClass\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hHwHfwa4Oks9","executionInfo":{"status":"ok","timestamp":1620308790074,"user_tz":-540,"elapsed":5513,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import random\n","\n","import cv2\n","\n","import pickle"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBfv8B6MPXQK"},"source":["전처리한 df [Object Detection.ipynb](https://github.com/hodurie/AI_Study/blob/master/Implementation/Datasets/Object%20Detection.ipynb)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"OyVDauEYO-6p","executionInfo":{"status":"ok","timestamp":1620308790076,"user_tz":-540,"elapsed":4441,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"993356f2-99a4-4d7a-e373-b551460ce7a6"},"source":["df = pd.read_csv('df.csv')\n","df.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>file_type</th>\n","      <th>object_length</th>\n","      <th>object</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>xmax</th>\n","      <th>ymax</th>\n","      <th>pose</th>\n","      <th>truncated</th>\n","      <th>difficult</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000012.jpg</td>\n","      <td>train</td>\n","      <td>1</td>\n","      <td>car</td>\n","      <td>156</td>\n","      <td>97</td>\n","      <td>351</td>\n","      <td>270</td>\n","      <td>Rear</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000017.jpg</td>\n","      <td>train</td>\n","      <td>2</td>\n","      <td>person</td>\n","      <td>185</td>\n","      <td>62</td>\n","      <td>279</td>\n","      <td>199</td>\n","      <td>Left</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000017.jpg</td>\n","      <td>train</td>\n","      <td>2</td>\n","      <td>horse</td>\n","      <td>90</td>\n","      <td>78</td>\n","      <td>403</td>\n","      <td>336</td>\n","      <td>Left</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>000023.jpg</td>\n","      <td>train</td>\n","      <td>6</td>\n","      <td>bicycle</td>\n","      <td>9</td>\n","      <td>230</td>\n","      <td>245</td>\n","      <td>500</td>\n","      <td>Unspecified</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>000023.jpg</td>\n","      <td>train</td>\n","      <td>6</td>\n","      <td>bicycle</td>\n","      <td>230</td>\n","      <td>220</td>\n","      <td>334</td>\n","      <td>500</td>\n","      <td>Frontal</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    file_name file_type  object_length  ...         pose  truncated  difficult\n","0  000012.jpg     train              1  ...         Rear          0          0\n","1  000017.jpg     train              2  ...         Left          0          0\n","2  000017.jpg     train              2  ...         Left          0          0\n","3  000023.jpg     train              6  ...  Unspecified          1          0\n","4  000023.jpg     train              6  ...      Frontal          1          0\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"u3pnIe21UD_y"},"source":["PASCAL VOC 2007 directory\n","```\n","/VOCdevkit//VOC2007/\n","├── df.csv                        # Object Detection 전처리 csv\n","├── seg.csv                       # Segmentation 전처리 csv\n","├── samples.pickle                # 선정 된 file 이름 pickle\n","└── Finetune                      # positive_list, negative_list txt 폴더\n","     ├── train                    # Segmentation 전처리 csv\n","     │   ├── bbox                # bbox csv 파일\n","     │   ├── *_n.csv / *_p.csv   # negative / positive csv (iou 0.5 기준 나눔)\n","     │   ├── Annotations         # negative / positive csv (iou 0.3 기준 나눔)\n","     │   ├── positive            # score 0.6 이상 bndboxes\n","     │        └── *_n.csv / *_p.csv\n","     │   └── bndboxes            # 기존 bndboxes\n","     │        └── *_n.csv / *_p.csv\n","     └── validation\n","          └── Annotations\n","\n","\n","# 기존 PASCAL VOC 2007 directory 구조\n","/VOCdevkit//VOC2007/\n","├── Annotations           \n","│   └── *.xml        \n","├── ImageSets\n","│   ├── Layout\n","│   │   └── *.txt\n","│   ├── Main\n","│   │   └── *.txt\n","│   └── Segmentation\n","│        └── *.txt\n","├── JPEGImages\n","│   └── .jpg\n","├── SegmentationClass\n","│   └── *.png\n","└── SegmentationObject\n","     └── *.png\n","\n","```\n"]},{"cell_type":"code","metadata":{"id":"ZhbD143eTpxD","executionInfo":{"status":"ok","timestamp":1620308856038,"user_tz":-540,"elapsed":1149,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["finetune_root_dir = './Finetune/'\n","\n","if not os.path.exists(finetune_root_dir):\n","    os.mkdir(finetune_root_dir)\n","\n","for name in ['train', 'validation']:\n","    dst_root_dir = os.path.join(finetune_root_dir, name)\n","\n","    if not os.path.exists(dst_root_dir):\n","        os.mkdir(dst_root_dir)\n","\n","    dst_annotation_dir = os.path.join(finetune_root_dir, name, 'Annotations')\n","    if not os.path.exists(dst_annotation_dir):\n","        os.mkdir(dst_annotation_dir)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lnPlpt168IR9"},"source":["특정 obj 만 추출해서 sample 만들기"]},{"cell_type":"code","metadata":{"id":"OLDGmksvtR5s","executionInfo":{"status":"ok","timestamp":1620308865871,"user_tz":-540,"elapsed":736,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["def sample_split(df, obj='car'):\n","    # car dataset 사용\n","    cond = df['object'] == obj\n","    df = df[cond]\n","\n","    samples = {}\n","\n","    for name in ['train', 'validation']:\n","        cond = df['file_type'] == name\n","        sample = df.loc[cond, 'file_name'].unique()\n","\n","        length = len(sample)\n","        \n","        indices = random.sample(range(length), int(length / 2))\n","        \n","        samples[name] = sample[indices]\n","\n","    return samples"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"m220Q3t9wSIq","executionInfo":{"status":"ok","timestamp":1620308870361,"user_tz":-540,"elapsed":1134,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["def IoU(pred_box, target_box):\n","    '''\n","    pred_box = [4] \n","    target_box = [N, 4]\n","    '''\n","    # (xmax - xmin) * (ymax - ymin)\n","\n","    if len(target_box.shape) == 1:\n","        target_box = target_box[np.newaxis, :]\n","\n","    areaA = (target_box[:, 2] - target_box[:, 0]) * (target_box[:, 3] - target_box[:, 1])\n","    areaB = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n","\n","    xA = np.maximum(pred_box[0], target_box[:, 0])\n","    yA = np.maximum(pred_box[1], target_box[:, 1])\n","    xB = np.minimum(pred_box[2], target_box[:, 2])\n","    yB = np.minimum(pred_box[3], target_box[:, 3])\n","    \n","    intersection = np.maximum(0.0, xB - xA) * np.maximum(0.0, yB - yA)\n","    \n","    scores = intersection / (areaA + areaB - intersection)\n","\n","    return scores"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"GmlkJ8tTwy2s","executionInfo":{"status":"ok","timestamp":1620308874441,"user_tz":-540,"elapsed":1048,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["def region_proposals(jpg, gs):\n","    global df\n","    path = os.path.join('./JPEGImages/', jpg)\n","    img = cv2.imread(path)\n","\n","    gs.setBaseImage(img)\n","    gs.switchToSelectiveSearchQuality()\n","\n","    rects = gs.process()\n","    # rects[N, 4]\n","    # [[xmin, ymin, xmax, ymax],\n","    # [xmin, ymin, xmax, ymax], ... ]\n","\n","    cond = df['file_name'] == jpg\n","    cols = ['xmin', 'ymin', 'xmax', 'ymax']\n","    bndboxes = np.array(df.loc[cond, cols])\n","\n","    maximum_bndbox_size = 0\n","\n","    for bndbox in bndboxes:\n","        xmin, ymin, xmax, ymax = bndbox\n","        bndbox_size = (xmax - xmin) * (ymax - ymin)\n","        if bndbox_size > maximum_bndbox_size:\n","            maximum_bndbox_size = bndbox_size\n","\n","    iou_list = []\n","    for rect in rects:\n","        scores = IoU(rect, bndboxes)\n","        iou_list.append(max(scores))\n","    \n","    return iou_list, rects, maximum_bndbox_size, bndboxes"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hsD6Xe0Flf0","executionInfo":{"status":"ok","timestamp":1620308876569,"user_tz":-540,"elapsed":966,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["def parse_annotation_jpg(jpg, gs):\n","    iou_list, rects, maximum_bndbox_size, bndboxes = region_proposals(jpg, gs)\n","\n","    positive_list = []\n","    negative_list = []\n","\n","    for i in range(len(iou_list)):\n","        xmin, ymin, xmax, ymax = rects[i]\n","        rect_size = (xmax - xmin) * (ymax - ymin)\n","\n","        iou_score = iou_list[i]\n","\n","        if iou_score >= 0.5:\n","            positive_list.append(rects[i])\n","        \n","        if 0 < iou_score < 0.5 and rect_size > (maximum_bndbox_size / 5.0):\n","            negative_list.append(rects[i])\n","    \n","    return positive_list, negative_list"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_e_8EAS8XlU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620311857200,"user_tz":-540,"elapsed":2923983,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"c9551743-7127-4c72-b0fa-ac6ea3d46586"},"source":["gs = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","\n","samples = sample_split(df)\n","\n","for name in ['train', 'validation']:\n","    total_num_positive = 0\n","    total_num_negative = 0\n","    \n","    for sample in samples[name]:\n","        positive_list, negative_list = parse_annotation_jpg(sample, gs)\n","        total_num_positive += len(positive_list)\n","        total_num_negative += len(negative_list)\n","\n","        finetune_path = os.path.join(finetune_root_dir, name)\n","\n","        positive_list_path = os.path.join(finetune_path, sample.replace('.jpg', '_p.csv'))\n","        negative_list_path = os.path.join(finetune_path, sample.replace('.jpg', '_n.csv'))\n","        \n","        np.savetxt(positive_list_path, np.array(positive_list), fmt='%d', delimiter=' ')\n","        np.savetxt(negative_list_path, np.array(negative_list), fmt='%d', delimiter=' ')\n","\n","    print('%s positive num: %d' % (name, total_num_positive))\n","    print('%s negative num: %d' % (name, total_num_negative))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n"],"name":"stderr"},{"output_type":"stream","text":["train positive num: 9658\n","train negative num: 67121\n","validation positive num: 7425\n","validation negative num: 54730\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a3OPLa7ztb3Y","executionInfo":{"status":"ok","timestamp":1620311857986,"user_tz":-540,"elapsed":769,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["with open('samples.pickle','wb') as fw:\n","    pickle.dump(samples, fw)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"V209Gly2t4XT","executionInfo":{"status":"ok","timestamp":1620311857987,"user_tz":-540,"elapsed":763,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["with open('samples.pickle', 'rb') as fr:\n","    samples = pickle.load(fr)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0ltv0AMxTiR","executionInfo":{"status":"ok","timestamp":1620311857988,"user_tz":-540,"elapsed":758,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"85ab2217-20c3-4e15-881d-465cd36b5b77"},"source":["samples"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'train': array(['006748.jpg', '008784.jpg', '003083.jpg', '001937.jpg',\n","        '003806.jpg', '006822.jpg', '002559.jpg', '000153.jpg',\n","        '003971.jpg', '003484.jpg', '003551.jpg', '004687.jpg',\n","        '005090.jpg', '002490.jpg', '008388.jpg', '002134.jpg',\n","        '008037.jpg', '002116.jpg', '005499.jpg', '008197.jpg',\n","        '000431.jpg', '001780.jpg', '009283.jpg', '008232.jpg',\n","        '007479.jpg', '003261.jpg', '001414.jpg', '006734.jpg',\n","        '000888.jpg', '003355.jpg', '008391.jpg', '008549.jpg',\n","        '001494.jpg', '004481.jpg', '006079.jpg', '008174.jpg',\n","        '003420.jpg', '005047.jpg', '009392.jpg', '009424.jpg',\n","        '000522.jpg', '001112.jpg', '006224.jpg', '000590.jpg',\n","        '007898.jpg', '004091.jpg', '000134.jpg', '002625.jpg',\n","        '003231.jpg', '008909.jpg', '005831.jpg', '003103.jpg',\n","        '000026.jpg', '008466.jpg', '007305.jpg', '001881.jpg',\n","        '004705.jpg', '006931.jpg', '002478.jpg', '001119.jpg',\n","        '009810.jpg', '000754.jpg', '002170.jpg', '004231.jpg',\n","        '009058.jpg', '007821.jpg', '001384.jpg', '009839.jpg',\n","        '005071.jpg', '000083.jpg', '006706.jpg', '008676.jpg',\n","        '003027.jpg', '007003.jpg', '009938.jpg', '009614.jpg',\n","        '002804.jpg', '000311.jpg', '005738.jpg', '006660.jpg',\n","        '009904.jpg', '005065.jpg', '000860.jpg', '008093.jpg',\n","        '003608.jpg', '006417.jpg', '004387.jpg', '001902.jpg',\n","        '000320.jpg', '005387.jpg', '008517.jpg', '003835.jpg',\n","        '000262.jpg', '009776.jpg', '009848.jpg', '009000.jpg',\n","        '004793.jpg', '008026.jpg', '000296.jpg', '001409.jpg',\n","        '004808.jpg', '008969.jpg', '007285.jpg', '009269.jpg',\n","        '000406.jpg', '004303.jpg', '008966.jpg', '001258.jpg',\n","        '000317.jpg', '004526.jpg', '009596.jpg', '009729.jpg',\n","        '000474.jpg', '008891.jpg', '005262.jpg', '003007.jpg',\n","        '000367.jpg', '009078.jpg', '006009.jpg', '000977.jpg',\n","        '007736.jpg', '006438.jpg', '002247.jpg', '004136.jpg',\n","        '000047.jpg', '007468.jpg', '002759.jpg', '000469.jpg',\n","        '005547.jpg', '001455.jpg', '007446.jpg', '002153.jpg',\n","        '001488.jpg', '008939.jpg', '007905.jpg', '008478.jpg',\n","        '008929.jpg', '004604.jpg', '008397.jpg', '005609.jpg',\n","        '007159.jpg', '002779.jpg', '008315.jpg', '008727.jpg',\n","        '001576.jpg', '007566.jpg', '002544.jpg', '000477.jpg',\n","        '008663.jpg', '009879.jpg', '003998.jpg', '006320.jpg',\n","        '005273.jpg', '008360.jpg', '009959.jpg', '000334.jpg',\n","        '001057.jpg', '001604.jpg', '003092.jpg', '002411.jpg',\n","        '002420.jpg', '005806.jpg', '009106.jpg', '000541.jpg',\n","        '002197.jpg', '004228.jpg', '001662.jpg', '006900.jpg',\n","        '003363.jpg', '008633.jpg', '005259.jpg', '000355.jpg',\n","        '000906.jpg', '009409.jpg', '003051.jpg', '004830.jpg',\n","        '004823.jpg', '004747.jpg', '003214.jpg', '004019.jpg',\n","        '005585.jpg', '002647.jpg', '009834.jpg', '007490.jpg',\n","        '004748.jpg', '005592.jpg', '009121.jpg', '001385.jpg',\n","        '009845.jpg', '004563.jpg', '006250.jpg', '006524.jpg',\n","        '003790.jpg', '001060.jpg', '001950.jpg', '009336.jpg',\n","        '008098.jpg', '009205.jpg', '008449.jpg', '003936.jpg',\n","        '001676.jpg'], dtype=object),\n"," 'validation': array(['008057.jpg', '003032.jpg', '002854.jpg', '007284.jpg',\n","        '005486.jpg', '009745.jpg', '006625.jpg', '003655.jpg',\n","        '003798.jpg', '006218.jpg', '005305.jpg', '001472.jpg',\n","        '003390.jpg', '005998.jpg', '000329.jpg', '006133.jpg',\n","        '001561.jpg', '007383.jpg', '008484.jpg', '001093.jpg',\n","        '002566.jpg', '009186.jpg', '005897.jpg', '000131.jpg',\n","        '007931.jpg', '001466.jpg', '007283.jpg', '009900.jpg',\n","        '004618.jpg', '009116.jpg', '006151.jpg', '001931.jpg',\n","        '005979.jpg', '007779.jpg', '008680.jpg', '001148.jpg',\n","        '008461.jpg', '006918.jpg', '002994.jpg', '006396.jpg',\n","        '005199.jpg', '005003.jpg', '000318.jpg', '002776.jpg',\n","        '008105.jpg', '007376.jpg', '001125.jpg', '009737.jpg',\n","        '002393.jpg', '000787.jpg', '000776.jpg', '009699.jpg',\n","        '000343.jpg', '009254.jpg', '007691.jpg', '008524.jpg',\n","        '006346.jpg', '002340.jpg', '008848.jpg', '008801.jpg',\n","        '003820.jpg', '001693.jpg', '009517.jpg', '000245.jpg',\n","        '006588.jpg', '000543.jpg', '007427.jpg', '009641.jpg',\n","        '000461.jpg', '000289.jpg', '001371.jpg', '007270.jpg',\n","        '000251.jpg', '000007.jpg', '006018.jpg', '008739.jpg',\n","        '001292.jpg', '009015.jpg', '005102.jpg', '006124.jpg',\n","        '006201.jpg', '009801.jpg', '003195.jpg', '004903.jpg',\n","        '008892.jpg', '006668.jpg', '006632.jpg', '006235.jpg',\n","        '009558.jpg', '002833.jpg', '003587.jpg', '009368.jpg',\n","        '002563.jpg', '001281.jpg', '000911.jpg', '008843.jpg',\n","        '009932.jpg', '003285.jpg', '007614.jpg', '007647.jpg',\n","        '009163.jpg', '000338.jpg', '008031.jpg', '007527.jpg',\n","        '002248.jpg', '006330.jpg', '005209.jpg', '002454.jpg',\n","        '002174.jpg', '000509.jpg', '000579.jpg', '002045.jpg',\n","        '001877.jpg', '008601.jpg', '003461.jpg', '005791.jpg',\n","        '007167.jpg', '003228.jpg', '004539.jpg', '009060.jpg',\n","        '002772.jpg', '005956.jpg', '007971.jpg', '001618.jpg',\n","        '007004.jpg', '006062.jpg', '006783.jpg', '004186.jpg',\n","        '004203.jpg', '009507.jpg', '002125.jpg', '004284.jpg',\n","        '003954.jpg', '000373.jpg', '001432.jpg', '008586.jpg',\n","        '000221.jpg', '005760.jpg', '009174.jpg', '002606.jpg',\n","        '002643.jpg', '004850.jpg', '008376.jpg', '001801.jpg',\n","        '005577.jpg', '004011.jpg', '009477.jpg', '001962.jpg',\n","        '000800.jpg', '007374.jpg', '003344.jpg', '009326.jpg',\n","        '005645.jpg', '006884.jpg', '002493.jpg', '003243.jpg',\n","        '003057.jpg', '007856.jpg', '001386.jpg', '000396.jpg',\n","        '009178.jpg', '005747.jpg', '004329.jpg', '007422.jpg',\n","        '008294.jpg', '007247.jpg', '007208.jpg', '002244.jpg',\n","        '001746.jpg', '005350.jpg', '009179.jpg', '007650.jpg',\n","        '006484.jpg', '003054.jpg', '000515.jpg', '000663.jpg',\n","        '002693.jpg', '000142.jpg', '007068.jpg'], dtype=object)}"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"jvjkxiVqsS3Y","executionInfo":{"status":"ok","timestamp":1620311857989,"user_tz":-540,"elapsed":758,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}}},"source":["def parse_annotation_jpeg_svm(jpg, gs):\n","    iou_list, rects, maximum_bndbox_size, bndboxs = region_proposals(jpg, gs)\n","\n","    positive_list = []\n","    negative_list = []\n","\n","    for i in range(len(iou_list)):\n","        xmin, ymin, xmax, ymax = rects[i]\n","        rect_size = (ymax - ymin) * (xmax - xmin)\n","\n","        iou_score = iou_list[i]\n","\n","        if 0 < iou_score <= 0.3 and rect_size > maximum_bndbox_size / 5.0:\n","            negative_list.append(rects[i])\n","        \n","    return bndboxs, negative_list"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrnYfVMiAGF0"},"source":["gs = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","\n","for name in ['train', 'validation']:\n","    total_num_positive = 0\n","    total_num_negative = 0\n","\n","    for sample in samples[name]:\n","        positive_list, negative_list = parse_annotation_jpeg_svm(sample, gs)\n","        total_num_positive += len(positive_list)\n","        total_num_negative += len(negative_list)\n","\n","        path = os.path.join(finetune_root_dir, name, 'Annotations')\n","        dst_annotation_positive_path = os.path.join(path, sample.replace('.jpg', '_p.csv'))\n","        dst_annotation_negative_path = os.path.join(path, sample.replace('.jpg', '_n.csv'))\n","\n","        np.savetxt(dst_annotation_positive_path, np.array(positive_list), fmt='%d', delimiter=' ')\n","        np.savetxt(dst_annotation_negative_path, np.array(negative_list), fmt='%d', delimiter=' ')\n","    \n","    print('%s positive num: %d' % (name, total_num_positive))\n","    print('%s negative num: %d' % (name, total_num_negative))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FwBVKoF4I22j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620306005633,"user_tz":-540,"elapsed":1764,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"0aebd33f-d3be-4c78-bf88-fd42176bbff7"},"source":["res_samples = []\n","total_positive_num = 0\n","\n","name = 'train'\n","\n","bbox_csv_path = os.path.join(finetune_root_dir, name, 'bbox')\n","bndboxes_path = os.path.join(finetune_root_dir, name, 'bndboxes')\n","positive_path = os.path.join(finetune_root_dir, name, 'positive')\n","\n","if not os.path.exists(bndboxes_csv_path):\n","    os.mkdir(bndboxes_csv_path)\n","\n","if not os.path.exists(positive_csv_path):\n","    os.mkdir(positive_csv_path)\n","\n","for sample in samples[name]:\n","    # ex) 'Finetune/train/00001_p.csv\n","    path = os.path.join(finetune_root_dir, name, sample.replace('.jpg', '_p.csv'))\n","    # positive_bndboxes = [N, 4]\n","    positive_bndboxes = np.loadtxt(path, dtype=np.int, delimiter=' ')\n","\n","    cols = ['xmin', 'ymin', 'xmax', 'ymax']\n","    train_df = df[df['file_name'] == sample]\n","    train_car = train_df.loc[df['object'] == 'car']\n","\n","    # bndboxes = [N, 4]\n","    bndboxes = np.array(train_car[cols])\n","    \n","    positive_list = []\n","\n","    # positive_bndboxes가 값을 하나 갖고 있을 때 (4)\n","    if len(positive_bndboxes.shape) == 1 and len(positive_bndboxes) != 0:\n","        scores = IoU(positive_bndboxes, bndboxes)\n","        if np.max(scores) > 0.6:\n","            positive_list.append(positive_bndboxes)\n","    # positive_bndboxes가 값을 여러 개 갖고 있을 때 (N, 4)\n","    elif len(positive_bndboxes.shape) == 2:\n","        for positive_bndbox in positive_bndboxes:\n","            scores = IoU(positive_bndbox, bndboxes)\n","            if np.max(scores) > 0.6:\n","                positive_list.append(positive_bndbox)\n","\n","    if len(positive_list) > 0:\n","        # ex) 'Finetune/train/bndboxes/00001.csv\n","        bndboxes_paths = os.path.join(bndboxes_path, sample.replace('jpg', 'csv'))\n","        np.savetxt(bndboxes_paths, bndboxes, fmt='%s', delimiter=' ')\n","\n","        # ex) 'Finetune/train/positive/00001.csv\n","        positive_csv_paths = os.path.join(positive_csv_path, sample.replace('jpg', 'csv'))\n","        np.savetxt(positive_csv_paths, np.array(positive_list), fmt='%s', delimiter=' ')\n","\n","        total_positive_num += len(positive_list)\n","        res_samples.append(sample)\n","        print('save {} done'.format(sample))\n","    else:\n","        print('{} ineligible'.format(sample))\n","\n","bnd_csv_path = os.path.join(bbox_csv_path, 'df.csv')\n","np.savetxt(bnd_csv_path, res_samples, fmt='%s', delimiter=' ')\n","print('total positive num: {}'.format(total_positive_num))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["-------- 004544.jpg ineligible\n","-------- 000431.jpg ineligible\n","-------- 001902.jpg ineligible\n","save 005483.jpg done\n","save 008633.jpg done\n","save 006524.jpg done\n","save 006900.jpg done\n","save 002704.jpg done\n","save 009336.jpg done\n","save 009406.jpg done\n","save 009434.jpg done\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/001902_p.csv\"\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"stream","text":["-------- 008037.jpg ineligible\n","save 002544.jpg done\n","save 001334.jpg done\n","save 005047.jpg done\n","-------- 000296.jpg ineligible\n","-------- 006104.jpg ineligible\n","save 003007.jpg done\n","save 003103.jpg done\n","save 000083.jpg done\n","-------- 008676.jpg ineligible\n","-------- 004973.jpg ineligible\n","-------- 001060.jpg ineligible\n","-------- 004808.jpg ineligible\n","-------- 003835.jpg ineligible\n","save 008001.jpg done\n","-------- 007736.jpg ineligible\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/004973_p.csv\"\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/001060_p.csv\"\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/003835_p.csv\"\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/007736_p.csv\"\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/002783_p.csv\"\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/005756_p.csv\"\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"stream","text":["save 006369.jpg done\n","-------- 000906.jpg ineligible\n","-------- 007446.jpg ineligible\n","save 001881.jpg done\n","-------- 002783.jpg ineligible\n","-------- 005756.jpg ineligible\n","-------- 008482.jpg ineligible\n","save 001954.jpg done\n","-------- 006748.jpg ineligible\n","save 004873.jpg done\n","save 002311.jpg done\n","-------- 004830.jpg ineligible\n","save 001604.jpg done\n","total positive num: 829\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/008482_p.csv\"\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: loadtxt: Empty input file: \"./Finetune/train/006748_p.csv\"\n","  del sys.path[0]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"StBRteg2YoRE"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.utils.data as data\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztx-68tEG0do"},"source":["class CustomFinetuneDataset():\n","    def __init__(self, root_dir, transform=None):\n","        '''\n","        root_dir = 'Finetune/train'\n","        jpg_path = 'JPEGImages/'\n","        '''\n","        with open('samples.pickle', 'rb') as fr:\n","            samples = pickle.load(fr)\n","\n","        name = root_dir.split('/')[-1]\n","\n","        jpg_path = './JPEGImages/'\n","        \n","        images = [cv2.imread(os.path.join(jpg_path, sample)) for sample in samples[name]]\n","\n","        annotations_path = os.path.join(root_dir, 'Annotations')\n","        annotations_dir = os.listdir(annotations_path)\n","    \n","        positive_annotations = [annotation for annotation in annotations_dir if annotation.endswith('_p.csv')]\n","        negative_annotations = [annotation for annotation in annotations_dir if annotation.endswith('_n.csv')]\n","        \n","        positive_sizes = []\n","        negative_sizes = []\n","        \n","        positive_rects = []\n","        negative_rects = []\n","\n","        for annotation in positive_annotations:\n","            rects = np.loadtxt(os.path.join(annotations_path, annotation), dtype=np.int, delimiter=' ')\n","\n","            if len(rects.shape) == 1:\n","                if rects.shape[0] == 4:\n","                    positive_rects.append(rects)\n","                    positive_sizes.append(1)\n","                else:\n","                    positive_sizes.append(0)\n","            else:\n","                positive_rects.extend(rects)\n","                positive_sizes.append(len(rects))\n","        \n","        for annotation in negative_annotations:\n","            rects = np.loadtxt(os.path.join(annotations_path, annotation), dtype=np.int, delimiter=' ')\n","            if len(rects.shape) == 1:\n","                if rects.shape[0] == 4:\n","                    negative_rects.append(rects)\n","                    negative_sizes.append(1)\n","                else:\n","                    positive_sizes.append(0)\n","            else:\n","                negative_rects.extend(rects)\n","                negative_sizes.append(len(rects))\n","\n","        self.transform = transform\n","        self.images = images\n","        self.positive_sizes = positive_sizes\n","        self.negative_sizes = negative_sizes\n","        self.positive_rects = positive_rects\n","        self.negative_rects = negative_rects\n","        self.total_positive_num = int(np.sum(positive_sizes))\n","        self.total_negative_num = int(np.sum(negative_sizes))\n","    \n","    def __getitem__(self, index):\n","        image_id = len(self.images) - 1\n","        if index < self.total_positive_num:\n","            target = 1\n","            xmin, ymin, xmax, ymax = self.positive_rects[index]\n","\n","            for i in range(len(self.positive_sizes) - 1):\n","                if np.sum(self.positive_sizes[:i]) <= index < np.sum(self.positive_sizes[:(i + 1)]):\n","                    image_id = i\n","                    break\n","            image = self.images[image_id][ymin:ymax, xmin:xmax]\n","        else:\n","            target = 0\n","            idx = index - self.total_positive_num\n","            xmin, ymin, xmax, ymax = self.negative_rects[idx]\n","\n","            for i in range(len(self.negative_sizes) - 1):\n","                if np.sum(self.negative_sizes[:i]) <= idx < np.sum(self.negative_sizes[:(i + 1)]):\n","                    image_id = i\n","                    break\n","            image = self.jpeg_images[image_id][ymin:ymax, xmin:xmax]\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return self.total_positive_num + self.total_negative_num\n","\n","    def get_positive_num(self):\n","        return self.total_positive_num\n","\n","    def get_negative_num(self):\n","        return self.total_negative_num"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTdtMELPpwiD"},"source":["from google.colab.patches import cv2_imshow\n","\n","def test_finetune(idx):\n","    root_dir = './Finetune/train'\n","    train_data_set = CustomFinetuneDataset(root_dir)\n","\n","    print('positive num: %d' % train_data_set.get_positive_num())\n","    print('negative num: %d' % train_data_set.get_negative_num())\n","    print('total num: %d' % train_data_set.__len__())\n","\n","    image, target = train_data_set.__getitem__(idx)\n","    print('target: %d' % target)\n","\n","    cv2_imshow(image)\n","    cv2.waitKey(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUXL9MS6V4-B"},"source":["test_finetune(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97lJOGyYZRWP"},"source":["class CustomClassifierDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        '''\n","        root_dir = 'Finetune/validation'\n","        jpg_path = 'JPEGImages/'\n","        '''\n","        with open('samples.pickle', 'rb') as fr:\n","            samples = pickle.load(fr)\n","\n","        images = []\n","        positive_list = []\n","        negative_list = []\n","\n","        name = root_dir.split('/')[-1]\n","\n","        jpg_path = 'JPEGImages/'\n","\n","        for idx in range(len(samples[name])):\n","            sample = samples[name][idx]\n","            images.append(cv2.imread(os.path.join(jpg_path, sample)))\n","\n","            positive_annotation_path = os.path.join(root_dir, 'Annotations', sample.replace('.jpg', '_p.csv'))\n","            positive_annotations = np.loadtxt(positive_annotation_path, dtype=np.int, delimiter=' ')\n","    \n","            if len(positive_annotations.shape) == 1:\n","                if positive_annotations.shape[0] == 4:\n","                    positive_dict = dict()\n","\n","                    positive_dict['rect'] = positive_annotations\n","                    positive_dict['image_id'] = idx\n","                    positive_list.append(positive_dict)\n","            else:\n","                for positive_annotation in positive_annotations:\n","                    positive_dict = dict()\n","\n","                    positive_dict['rect'] = positive_annotation\n","                    positive_dict['image_id'] = idx\n","                    positive_list.append(positive_dict)\n","\n","            negative_annotation_path = os.path.join(root_dir, 'Annotations', sample.replace('.jpg', '_p.csv'))\n","            negative_annotations = np.loadtxt(negative_annotation_path, dtype=np.int, delimiter=' ')\n","\n","            if len(negative_annotations.shape) == 1:\n","                if negative_annotations.shape[0] == 4:\n","                    negative_dict = dict()\n","                    negative_dict['rect'] = negative_annotations\n","                    negative_dict['image_id'] = idx\n","                    negative_list.append(negative_dict)\n","            else:\n","                for negative_annotation in negative_annotations:\n","                    negative_dict = dict()\n","\n","                    negative_dict['rect'] = negative_annotation\n","                    negative_dict['image_id'] = idx\n","                    negative_list.append(negative_dict)\n","\n","        self.transform = transform\n","        self.images = images\n","        self.positive_list = positive_list\n","        self.negative_list = negative_list\n","\n","    def __getitem__(self, index):\n","        if index < len(self.positive_list):\n","            target = 1\n","            positive_dict = self.positive_list[index]\n","\n","            xmin, ymin, xmax, ymax = positive_dict['rect']\n","            image_id = positive_dict['image_id']\n","\n","            image = self.images[image_id][ymin:ymax, xmin:xmax]\n","            cache_dict = positive_dict\n","        else:\n","            target = 0\n","            idx = index - len(self.positive_list)\n","            negative_dict = self.negative_list[idx]\n","\n","            xmin, ymin, xmax, ymax = negative_dict['rect']\n","            image_id = negative_dict['image_id']\n","\n","            image = self.images[image_id][ymin:ymax, xmin:xmax]\n","            cache_dict = negative_dict\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, target, cache_dict\n","\n","    def __len__(self):\n","        return len(self.positive_list) + len(self.negative_list)\n","\n","    def get_transform(self):\n","        return self.transform\n","\n","    def get_jpeg_images(self):\n","        return self.images\n","\n","    def get_positive_num(self):\n","        return len(self.positive_list)\n","\n","    def get_negative_num(self):\n","        return len(self.negative_list)\n","\n","    def get_positives(self):\n","        return self.positive_list\n","\n","    def get_negatives(self):\n","        return self.negative_list\n","\n","    def set_negative_list(self, negative_list):\n","        self.negative_list = negative_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igO0YTHax2g0"},"source":["from google.colab.patches import cv2_imshow\n","\n","def test_classifier(idx):\n","    root_dir = 'Finetune/validation'\n","    train_data_set = CustomClassifierDataset(root_dir)\n","\n","    print('positive num: %d' % train_data_set.get_positive_num())\n","    print('negative num: %d' % train_data_set.get_negative_num())\n","    print('total num: %d' % train_data_set.__len__())\n","\n","    image, target, cache_dict = train_data_set.__getitem__(idx)\n","    print('target: %d' % target)\n","    print('dict: ' + str(cache_dict))\n","\n","    cv2_imshow(image)\n","    cv2.waitKey(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138},"id":"4HrMT9vGxy_2","executionInfo":{"status":"ok","timestamp":1620306592378,"user_tz":-540,"elapsed":942,"user":{"displayName":"SH","photoUrl":"","userId":"04796031619100753677"}},"outputId":"b82175ad-441e-41ae-deca-499e0209a57a"},"source":["test_classifier(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["positive num: 92\n","negative num: 92\n","total num: 184\n","target: 1\n","dict: {'rect': array([324, 207, 385, 238]), 'image_id': 0}\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<PIL.Image.Image image mode=RGB size=61x31 at 0x7FE4AE772210>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"LsZLVlQ1zLKE"},"source":["class BBoxRegressionDataset():\n","    def __init__(self, root_dir, transform=None):\n","        '''\n","        root_dir = 'Finetune/train'\n","        \n","        bndboxes path = 'Finetune/train/bndboxes'\n","        positive path = 'Finetune/train/positive'\n","        두 파일과 관련 된 csv = 'Finetune/train/bbox'\n","        '''\n","        super(BBoxRegressionDataset, self).__init__()\n","        self.transform = transform\n","\n","        name = root.split('/')[-1]\n","        txt = np.loadtxt(os.path.join(root_dir, 'bbox', 'df.csv'), dtype=np.str)\n","\n","        jpg_path = 'JPEGImages/'\n","\n","        images = []\n","        box_list = []\n","\n","        for i in range(len(txt)):\n","            sample = txt[i]\n","\n","            jpeg_path = os.path.join(jpg_path, sample)\n","            bndbox_path = os.path.join(root_dir, 'bndboxes', sample.replace('jpg', 'csv'))\n","            positive_path = os.path.join(root_dir, 'positive', sample.replace('jpg', 'csv'))\n","\n","            images.append(cv2.imread(jpeg_path))\n","            bndboxes = np.loadtxt(bndbox_path, dtype=np.int, delimiter=' ')\n","            positives = np.loadtxt(positive_path, dtype=np.int, delimiter=' ')\n","\n","            if len(positives.shape) == 1:\n","                bndbox = self.get_bndbox(bndboxes, positives)\n","                box_list.append({'image_id': i, 'positive': positives, 'bndbox': bndbox})\n","            else:\n","                for positive in positives:\n","                    bndbox = self.get_bndbox(bndboxes, positive)\n","                    box_list.append({'image_id': i, 'positive': positive, 'bndbox': bndbox})\n","\n","        self.images = images\n","        self.box_list = box_list\n","\n","    def __getitem__(self, index: int):\n","        assert index < self.__len__(), 'The data set size is %d, and the current input subscript is %d' % (self.__len__(), index)\n","\n","        box_dict = self.box_list[index]\n","        image_id = box_dict['image_id']\n","        positive = box_dict['positive']\n","        bndbox = box_dict['bndbox']\n","\n","        jpeg_img = self.images[image_id]\n","        xmin, ymin, xmax, ymax = positive\n","        image = jpeg_img[ymin:ymax, xmin:xmax]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        target = dict()\n","        p_w = xmax - xmin\n","        p_h = ymax - ymin\n","        p_x = xmin + p_w / 2\n","        p_y = ymin + p_h / 2\n","\n","        xmin, ymin, xmax, ymax = bndbox\n","        g_w = xmax - xmin\n","        g_h = ymax - ymin\n","        g_x = xmin + g_w / 2\n","        g_y = ymin + g_h / 2\n","\n","        t_x = (g_x - p_x) / p_w\n","        t_y = (g_y - p_y) / p_h\n","        t_w = np.log(g_w / p_w)\n","        t_h = np.log(g_h / p_h)\n","\n","        return image, np.array((t_x, t_y, t_w, t_h))\n","\n","    def __len__(self):\n","        return len(self.box_list)\n","\n","    def get_bndbox(self, bndboxes, positive):\n","        if len(bndboxes.shape) == 1:\n","            return bndboxes\n","        else:\n","            scores = IoU(positive, bndboxes)\n","            return bndboxes[np.argmax(scores)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ny41nUn2_OS"},"source":["import torchvision.transforms as transforms\n","\n","def test_bbox():\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((227, 227)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","    root_dir = 'Finetune/train'\n","    data_set = BBoxRegressionDataset(root_dir, transform=transform)\n","\n","    print(data_set.__len__())\n","    image, target = data_set.__getitem__(10)\n","    print(image.shape)\n","    print(target)\n","    print(target.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vzkcsq7IWYb"},"source":["test_bbox()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"np6649GFIXOJ"},"source":["from torch.utils.data import Sampler\n","\n","class CustomBatchSampler(Sampler):\n","    def __init__(self, num_positive, num_negative, batch_positive, batch_negative):\n","        \"\"\"\n","        2 classification data set\n","        Batch processing each time, including batch_positive positive samples and batch_negative negative samples\n","        @param num_positive: number of positive samples\n","        @param num_negative: number of negative samples\n","        @param batch_positive: the number of positive samples at a time\n","        @param batch_negative: number of negative samples at a time\n","        \"\"\"\n","        self.num_positive = num_positive\n","        self.num_negative = num_negative\n","        self.batch_positive = batch_positive\n","        self.batch_negative = batch_negative\n","\n","        length = num_positive + num_negative\n","        self.idx_list = list(range(length))\n","\n","        self.batch = batch_negative + batch_positive\n","        self.num_iter = length // self.batch\n","\n","    def __iter__(self):\n","        sampler_list = list()\n","        for i in range(self.num_iter):\n","            tmp = np.concatenate(\n","                (random.sample(self.idx_list[:self.num_positive], self.batch_positive),\n","                 random.sample(self.idx_list[self.num_positive:], self.batch_negative))\n","            )\n","            random.shuffle(tmp)\n","            sampler_list.extend(tmp)\n","        return iter(sampler_list)\n","\n","    def __len__(self) -> int:\n","        return self.num_iter * self.batch\n","\n","    def get_num_batch(self) -> int:\n","        return self.num_iter\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0SE90pjOPYr"},"source":["## Reference\n","- [R-CNN](https://github.com/object-detection-algorithm/R-CNN)\n","\n","```\n","@misc{girshick2013rich,\n","    title={Rich feature hierarchies for accurate object detection and semantic segmentation},\n","    author={Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},\n","    year={2013},\n","    eprint={1311.2524},\n","    archivePrefix={arXiv},\n","    primaryClass={cs.CV}\n","}\n","\n","@misc{pascal-voc-2007,\n","\tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n","\ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n","\thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\n","\n","```"]}]}
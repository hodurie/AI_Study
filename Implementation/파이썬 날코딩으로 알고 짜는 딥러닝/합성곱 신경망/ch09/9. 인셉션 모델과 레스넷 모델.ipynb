{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9. 인셉션 모델과 레스넷 모델.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMNXE2bZ8BlqcGdj8F1YJYG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4pSerdm_awto"},"source":["# 9. 인셉션 모델과 레스넷 모델"]},{"cell_type":"markdown","metadata":{"id":"7o5Pr7Paa1TF"},"source":["## 인셉션 모델\n","인셉션 모델은 인셉션 모듈이라고 하는 병렬 처리 합성곱 신경망 구조를 반복적으로 활용하여 신경망의 규모를 크게 늘린 모델이다.\n","\n","인셉션 모델은 이전 CNN 모델(AlexNet)보다 파라미터의 수를 줄이고 신경망의 깊이를 늘린 모델로 Google에서 개발하였다.\n","\n","인셉션 모델에 대한 다양한 내용들이 있지만 여기선 세 가지에 대해서만 알아볼 예정이다.\n","1. 인셉션 모델과 인셉션 모듈\n","2. 합성곱 연산 과정의 변형을 이용한 계산량 절감\n","3. 인셉션-v3(Inception-v3) 모델의 구조"]},{"cell_type":"markdown","metadata":{"id":"01qm2tXba4M5"},"source":["### 인셉션 모델과 인셉션 모듈\n","인셉션 모듈은 하나의 입력을 병렬로 처리해 다시 합쳐 하나의 출력으로 내보내 주는 역할을 한다.  \n","\n","이 과정을 자세히 알아보면  \n","\n","<img src='https://user-images.githubusercontent.com/25279765/35002290-8af62246-fb2c-11e7-8692-12b9ccfff216.jpg'>\n","\n","이전 layer에서 들어온 입력을 여러 분기로 나눠 conv layer을 통과 시킨다.  \n","네 분기는 각각 `1x1`, `3x3`, `5x5`, `3x3 max pooling`을 거친다.  \n","이는 한 가지 분석 방법에 전적으로 의존하기보단 여러 방법으로 분석하고 그 결과를 종합하는 편이 좋은 결과를 가져올 것이라는 믿음에 기반을 둔 접근 방법이다.  \n","인공지능 분야에서 이러한 접근을 앙상블 또는 배깅이라 한다.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UPmiINhFa5qQ"},"source":["### 합선곱 연산 과정의 변형\n","하지만 위의 인셉션 모듈엔 큰 단점이 하나 존재했다.  \n","그 단점은 층이 쌓여가며 각각의 conv layer에서 엄청난 계산량을 요구하게 된다.  \n","이를 보완하기 위해 기존 conv layer을 지나기 전 `1x1` conv layer을 추가함으로써 전체적인 계산량을 줄여 처리 속도를 높였다.  \n","\n","\n","<img src='https://mblogthumb-phinf.pstatic.net/20160418_20/laonple_1460944007653Qxk6M_PNG/041816_0146_3.png?type=w2'>"]},{"cell_type":"markdown","metadata":{"id":"u3y0ntYKa5tl"},"source":["### 인셉션-v3 모델의 구조\n","구글넷은 인셉션 모듈을 활용해서 v1부터 v4 까지 이루는 다양한 인셉션 모듈을 선보였다.  \n","\n","그 중 v3 모델 구조에 대해 알아보려 한다.  \n","아래는 v3 모델의 구조이다.  \n","\n","<img src='https://cloud.google.com/tpu/docs/images/cloudtputools--zmko2ewxpmt.png'>\n","\n","모델의 맨 앞과 맨 뒤는 병렬 처리 없이 직렬로 연결 돼 있으며 나머지는 인셉션 모듈로 구성 돼 있다.  "]},{"cell_type":"markdown","metadata":{"id":"dH0pxwNSa5xB"},"source":["## 레스넷 모델\n","레스넷 모델은 VGG 모델을 기반으로 모델 안에 레지듀얼 블록을 연결해 만들었다.  \n","\n","레지듀얼 모듈은 합성곱 계층의 처리 결과에 원래 입력을 더하는 방식을 이용해 만들었다.\n","\n","여기서도 간단하게 세 가지에 대해서만 알아보려 한다.  \n","1. 레스넷 모델의 전신이 된 VGG-19 모델과 플레인-34 모델\n","2. 레지듀얼 블록과 레지듀얼-34 모델\n","3. 보틀넥 블록과 레스넷-152 모델"]},{"cell_type":"markdown","metadata":{"id":"sKES80Bea5z9"},"source":["### VGG-19 모델과 플레인-34 모델\n","\n","VGG-19 모델은 16개의 conv layer와 5개의 pooling layer 그리고 3개의 완전 연결층으로 이루어져 있다.  \n","\n","플레인-34 모델은 conv layer의 개수를 33개로 늘리고 완전 연결층의 개수를 1개로 줄였다.  \n"]},{"cell_type":"markdown","metadata":{"id":"3P1LTP6ya524"},"source":["### 레지듀얼 블록과 레지듀얼-34 모델\n","레지듀얼-34 모델의 경우 위의 플레인-34 모델에 레지듀얼 블록을 추가한 모델이다.  \n","\n","<image src='https://media.vlpt.us/images/goe87088/post/75b09a60-76f5-47bd-b08a-266d8af81350/image.png'>\n","\n","플레인 블록의 출력을 $P(x)$ 라하고  \n","레지듀얼 블록의 출력을 $R(x)$ 라 할 때  \n","위의 식은 입력 $x$에 작은 변화가 더해진다는 의미에서 아래와 같이 표현할 수 있다.  \n","\n","$$P(x) = x + p(x)$$\n","$$R(x) = x + r(x)$$\n","\n","$P(x) = x + p(x)$의 식의 경우 두 개의 합성곱 계층이 $x+p(x)$ 전체를 만들어 내야하며 이는 $x$를 재현하는 동시에 추가로 $p(x)$까지 제대로 포착하도록 학습 되어야 한다는 의미이다.  \n","\n","반면 아래식 $R(x) = x + r(x)$의 경우에는 별로도 $x$가 더해지는 방식이므로 두 개의 합성곱이 작은 변화 $r(x)$에만 집중을 수행하게 하면 된다.  \n"]},{"cell_type":"markdown","metadata":{"id":"lWOtgIkfa579"},"source":["### 보틀넥 블록과 레스넷-152 모델\n","레지듀얼 블록에서 보틀넥 블록을 사용하였다.  \n","`1x1`의 conv layer을 추가함으로써 256 차원의 채널을 입력으로 받아 정보를 더 많이 얻으면서도 연산량은 이전보다 적게 가져가는 효과를 냈다.  \n","\n","이를 통해 기존의 모델의 크기를 152개와 같은 깊은 구조의 레스넷 모델들을 만들 수 있게 됐다.  \n","\n","<image src='https://i.stack.imgur.com/kbiIG.png'>"]},{"cell_type":"code","metadata":{"id":"tiHSO6qNa5_i"},"source":[""],"execution_count":null,"outputs":[]}]}
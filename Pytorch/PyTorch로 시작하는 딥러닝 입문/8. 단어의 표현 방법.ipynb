{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8. 단어의 표현 방법.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPU5a9CpTtCnCx3MjgUDpl7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"I1580UMX2tXD"},"source":["# PyTorch로 시작하는 딥러닝 입문\r\n","## 8. 단어의 표현 방법"]},{"cell_type":"markdown","metadata":{"id":"bZOXraD02yIP"},"source":["### NLP에서의 원-핫 인코딩\r\n","컴퓨터는 문자보다는 숫자를 더 잘 처리 하므로  \r\n","자연어 처리를 위해서는 문자를 숫자로 변환해야 함    \r\n"]},{"cell_type":"markdown","metadata":{"id":"mZtVO8lK3hNc"},"source":["#### 원-핫 인코딩이란?\r\n","원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고  \r\n","표현하고 싶은 단어의 인덱스를 1 나머지를 0으로 부여하는 단어의 벡터 표현 방식  \r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"w1x0VN1_2syf"},"source":["text = '나는 자연어 처리를 배운다'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA6-KHH333-E","executionInfo":{"status":"ok","timestamp":1610610632888,"user_tz":-540,"elapsed":6299,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"30eebe0e-4897-4334-b3f8-60aa0f866674"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Collecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Collecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n","\u001b[K     |████████████████████████████████| 460kB 49.9MB/s \n","\u001b[?25hCollecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 15.0MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: tweepy, colorama, JPype1, beautifulsoup4, konlpy\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEcD1B9l39Hd","executionInfo":{"status":"ok","timestamp":1610610670815,"user_tz":-540,"elapsed":7299,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"75b72ce5-2b15-4b1b-dd02-8c849e9b6881"},"source":["from konlpy.tag import Okt\r\n","okt = Okt()\r\n","token = okt.morphs(text)\r\n","print(token)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['나', '는', '자연어', '처리', '를', '배운다']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VBcQ7pKk4GHo","executionInfo":{"status":"ok","timestamp":1610610721325,"user_tz":-540,"elapsed":700,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"dd669b01-a1cc-4e98-ec66-757ac4f60417"},"source":["word2index = {}\r\n","for voca in token:\r\n","    if voca not in word2index.keys():\r\n","        word2index[voca] = len(word2index)\r\n","print(word2index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iQiEFit54UVG"},"source":["def one_hot_encoding(word, word2index):\r\n","    one_hot_vector = [0] * len(word2index)\r\n","    index = word2index[word]\r\n","    one_hot_vector[index] = 1\r\n","    return one_hot_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEwDk9-j4i5P","executionInfo":{"status":"ok","timestamp":1610610833785,"user_tz":-540,"elapsed":619,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"b9fd6e5c-290f-4fd1-cb70-c8a14a858f53"},"source":["one_hot_encoding(\"자연어\", word2index)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 1, 0, 0, 0]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"66tTZ43F4xpW"},"source":["#### 원-핫 인코딩의 한계\r\n","원-핫 인코딩의 경우 단어의 개수가 늘어날 수록 벡터를 저장할 공간이 계속 늘어나는 단점이 존재  \r\n","\r\n","또한, 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 존재  "]},{"cell_type":"markdown","metadata":{"id":"NbYSKrTq71EM"},"source":["### 워드 임베딩"]},{"cell_type":"markdown","metadata":{"id":"m9YqlqW172XJ"},"source":["#### 희소 표현\r\n","원-핫 인코딩을 통해 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스 값만 1이고 나머지는 0으로 표현 됨  \r\n","이렇게 벡터 또는 행렬의 값이 대부분 0으로 표현 되는 방법을 희소 표현이라고 함  "]},{"cell_type":"code","metadata":{"id":"nORosdZW7yzT"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wh9g7Bke8PDg"},"source":["dog = torch.FloatTensor([1, 0, 0, 0, 0])\r\n","cat = torch.FloatTensor([0, 1, 0, 0, 0])\r\n","computer = torch.FloatTensor([0, 0, 1, 0, 0])\r\n","netbook = torch.FloatTensor([0, 0, 0, 1, 0])\r\n","book = torch.FloatTensor([0, 0, 0, 0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iTI5qzr8aRL","executionInfo":{"status":"ok","timestamp":1610611836440,"user_tz":-540,"elapsed":658,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"c298f7b0-4faf-47c7-9162-d4212fdede37"},"source":["print(torch.cosine_similarity(dog, cat, dim=0))\r\n","print(torch.cosine_similarity(cat, computer, dim=0))\r\n","print(torch.cosine_similarity(computer, netbook, dim=0))\r\n","print(torch.cosine_similarity(netbook, book, dim=0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.)\n","tensor(0.)\n","tensor(0.)\n","tensor(0.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xtxWptvH8q59"},"source":["#### 밀집 표현\r\n","밀집 표현은 희소 표현과 반대되는 표현으로  \r\n","벡터의 차원을 단어 집합의 크기로 상정하지 않고 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤   \r\n","또한, 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됨"]},{"cell_type":"markdown","metadata":{"id":"HUgKrZ6Zbe9p"},"source":["#### 워드 임베딩\r\n","단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 **워드 임베딩(word embedding)** 이라 함  \r\n","밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 **임베딩 벡터(embedding vector)**라고도 함  \r\n","\r\n","||원-핫 벡터|임베딩 벡터\r\n","---|----|---\r\n","차원|고차원(단어 집합의 크기)|저차원\r\n","다른 표현|희소 벡터의 일종|밀집 벡터의 일종\r\n","표현 방법|수동|훈련 데이터로부터 학습함\r\n","값의 타입|1과 0|실수"]},{"cell_type":"markdown","metadata":{"id":"NIPHT-2ncHZw"},"source":["### 워드투벡터"]},{"cell_type":"markdown","metadata":{"id":"AWbDltAueVm7"},"source":["#### 희소 표현\r\n","희소 표현은 벡터 또는 행렬의 값이 대부분 0으로 표현 되는 것을 말하며  \r\n","희소 표현을 이용해 만든 벡터를 희소 벡터라 하며 이는 원-핫 벡터와 동일  \r\n","\r\n","하지만 이런 표현은 각 단어간 유사성을 표현 할 수 없다는 단점이 존재하며  \r\n","이를 위한 대안으로 단어의 `의미`를 다차원 공간에 벡터화 하는 방법을 찾았고  \r\n","이러한 표현을 **분산 표현(distributed representation)**이라고 함  \r\n","이때 분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업은 워드 임베딩 작업에 속하므로  \r\n","이렇게 표현 된 벡터 또한 임베딩 벡터라고 하며  \r\n","저차원을 가지므로 밀집 벡터에 속하기도 함  "]},{"cell_type":"markdown","metadata":{"id":"7rJRXnFefw5j"},"source":["#### 분산 표현\r\n","분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법으로  \r\n","`비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다`라는 가정임  \r\n","\r\n","분산 표현은 분포 가설을 이용하여 단어들의 셋을 학습하고  \r\n","벡터에 단어의 의미를 여러 차원에 분산하여 표현함  \r\n","이를 이용하면 기존의 단어 집합의 크기만큼 차원을 만들 필요가 없어 저차원의 형태로 표현할 수 있음  \r\n","또한 **단어의 의미를 여러 차원에 분산하여 표현**하여 **단어 간 유사도**를 구할 수 있음  "]},{"cell_type":"markdown","metadata":{"id":"UdNUGM9cg5Jz"},"source":["#### CBOW\r\n","Word2Vec에 CBOW는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법  \r\n","\r\n","중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지를 설정하는데  \r\n","이 설정한 범위를 **윈도우**라고 함  \r\n","\r\n","윈도우 크기를 정했다면   \r\n","윈도우를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 데이터 셋을 만듦  \r\n","이 방법을 **슬라이딩 윈도우** 라고 함  \r\n","\r\n","이때 슬라이딩 윈도우를 통해 주변 단어의 원-핫 벡터를 생성하고  \r\n","이를 인공 신경망에 넣어 값을 예측하고 이를 출력해 냄  \r\n","즉 `입력층 - 은닉층 - 출력층`을 거치는 인공 신경망으로 딥러닝 모델은 아님 \r\n","또한, 일반 은닉층과는 달리 활성화 함수를 거치지 않아 **투사층** 이라고도 부름  "]},{"cell_type":"markdown","metadata":{"id":"ipNgpEYHhRgc"},"source":["#### Skip-Gram\r\n","Word2Vec에 Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법  \r\n","Skip-gram은 CBOW의 메커니즘 자체는 동일하지만  \r\n","투사층에서 벡터들의 평균을 구하지 않음  \r\n","\r\n","여러 논문에서 성능 비교를 했을 때 전반적으로 Skip-Gram이 CBOW보다 성능이 좋다고 알려져 있음  \r\n"]},{"cell_type":"markdown","metadata":{"id":"zXYVG3Cg0dVq"},"source":["#### 네거티브 샘플링\r\n","대체적으로 Word2Vec를 사용한다고 하면 SGNS(Skip-Gram with Negative Sampling)을 사용  \r\n","Skip-gram을 사용하는데, 네거티브 샘플링(Negative Sampling)이란 방법까지 추가로 사용한다는 것  \r\n","\r\n","Word2Vec 모델에는 한가지 문제점이 있는데  \r\n","중심 단어와 주변 단어만 오차를 구하고 임베딩을 조절해야하지만  \r\n","상관없는 단어들까지 오차를 구하고 임베딩을 조절한다는 점  \r\n","그래서 속도가 느림  \r\n","\r\n","이를 효율적으로 만드는 방법은  \r\n","전체 단어 집합보다 훨씬 작은 단어 집합을 만들어  \r\n","주변 단어들을 긍정으로 두고  \r\n","랜덤 샘플링 된 단어들을 부정으로 둬  \r\n","이준 분류 문제를 수행하는 것  "]},{"cell_type":"markdown","metadata":{"id":"4HcUUIch1zd4"},"source":["### 영어/한국어 Word2Vec 훈련시키기\r\n"]},{"cell_type":"markdown","metadata":{"id":"cKP7R9aV1018"},"source":["#### 영어 Word2Vec 만들기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pVn8Rng142a","executionInfo":{"status":"ok","timestamp":1610626880972,"user_tz":-540,"elapsed":2217,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"c52185ee-bf1e-4d5c-e8ee-9c2aedafcc00"},"source":["import nltk\r\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"ECQN8pJg18_O"},"source":["import urllib.request\r\n","import zipfile\r\n","from lxml import etree\r\n","import re\r\n","from nltk.tokenize import word_tokenize, sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHOmdeYr2F_5","executionInfo":{"status":"ok","timestamp":1610626950779,"user_tz":-540,"elapsed":2013,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"e473e90b-ccae-43d5-ae67-8d4c792c0a8d"},"source":["# 훈련 데이터 이해하기\r\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7fe87c990400>)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"eXERaVf42OFg"},"source":["# 훈련 데이터 전처리하기\r\n","targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF-8')\r\n","target_text = etree.parse(targetXML)\r\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\r\n","\r\n","content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\r\n","sent_text = sent_tokenize(content_text)\r\n","\r\n","normalized_text = []\r\n","for string in sent_text:\r\n","    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\r\n","    normalized_text.append(tokens)\r\n","\r\n","result = []\r\n","result = [word_tokenize(sentence) for sentence in normalized_text]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVehKXEo38PC","executionInfo":{"status":"ok","timestamp":1610627437340,"user_tz":-540,"elapsed":34881,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"6c3bad1a-ecaf-4cb4-ab27-1deb53f877f3"},"source":["print('총 샘플의 개수 : {}'.format(len(result)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["총 샘플의 개수 : 273424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkyxegqT39_M","executionInfo":{"status":"ok","timestamp":1610627438431,"user_tz":-540,"elapsed":1084,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"b0b46953-2306-4ad9-8380-35626ee40779"},"source":["for line in result[:3]:\r\n","    print(line)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n","['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n","['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4jfXG9xI4ARM"},"source":["# Word2Vec 훈련시키기\r\n","from gensim.models import Word2Vec, KeyedVectors\r\n","model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AgfvkOHz60lm"},"source":["Word2Vec의 하이퍼파라미터값\r\n","- **size** = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\r\n","- **window** = 컨텍스트 윈도우 크기\r\n","- **min_count** = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\r\n","- **workers** = 학습을 위한 프로세스 수\r\n","- **sg** = 0은 CBOW, 1은 Skip-gram."]},{"cell_type":"markdown","metadata":{"id":"HL8ccdZ_76Ml"},"source":["'man'이랑 유사한 단어들 출력"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgdKKY3W615T","executionInfo":{"status":"ok","timestamp":1610628239958,"user_tz":-540,"elapsed":665,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"fb6e6395-8cda-473c-946c-57280aee6054"},"source":["model_result = model.wv.most_similar('man')\r\n","print(model_result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('woman', 0.866657018661499), ('guy', 0.8221429586410522), ('lady', 0.7798658609390259), ('girl', 0.7722408771514893), ('boy', 0.7629590034484863), ('gentleman', 0.7600579261779785), ('soldier', 0.733878493309021), ('poet', 0.6964517831802368), ('kid', 0.6944060325622559), ('king', 0.6563361287117004)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J9YeLjmh70BP"},"source":["# Word2Vec 모델 저장하고 로드하기\r\n","model.wv.save_word2vec_format('./eng_w2v')\r\n","loaded_model = KeyedVectors.load_word2vec_format('eng_w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjFMaZvI8SvJ","executionInfo":{"status":"ok","timestamp":1610628564271,"user_tz":-540,"elapsed":650,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"e59bb90a-100c-46df-b2d8-b3d2afae811a"},"source":["model_result = loaded_model.most_similar('man')\r\n","print(model_result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('woman', 0.866657018661499), ('guy', 0.8221429586410522), ('lady', 0.7798658609390259), ('girl', 0.7722408771514893), ('boy', 0.7629590034484863), ('gentleman', 0.7600579261779785), ('soldier', 0.733878493309021), ('poet', 0.6964517831802368), ('kid', 0.6944060325622559), ('king', 0.6563361287117004)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wLarYGj98dys"},"source":["#### 한국어 Word2Vec 만들기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"211H5K6G8if5","executionInfo":{"status":"ok","timestamp":1610628911456,"user_tz":-540,"elapsed":147596,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"840eaa9a-d5de-4db9-b580-ab80c1c388aa"},"source":["# 위키피디아 한국어 덤프 파일 다운로드\r\n","!wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-01-14 12:52:44--  https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\n","Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.155.106, 2620:0:861:4:208:80:155:106\n","Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.155.106|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 730816628 (697M) [application/octet-stream]\n","Saving to: ‘kowiki-latest-pages-articles.xml.bz2’\n","\n","kowiki-latest-pages 100%[===================>] 696.96M  4.67MB/s    in 2m 27s  \n","\n","2021-01-14 12:55:11 (4.75 MB/s) - ‘kowiki-latest-pages-articles.xml.bz2’ saved [730816628/730816628]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUXhwlfCCCC4","executionInfo":{"status":"ok","timestamp":1610630051751,"user_tz":-540,"elapsed":3768,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"81552d8b-6e6b-4e3d-b980-0b35e909b3f7"},"source":["# 위키피디아 익스트랙터 다운로드\r\n","!git clone \"https://github.com/attardi/wikiextractor.git\"  \r\n","!pip install wikiextractor"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wikiextractor\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/bd/6b8ffc89fa4abefd801f7b0f83bc17382664484bd32eb6529b243d7a8f12/wikiextractor-3.0.4-py3-none-any.whl (46kB)\n","\r\u001b[K     |███████                         | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 20.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 30kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 40kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n","\u001b[?25hInstalling collected packages: wikiextractor\n","Successfully installed wikiextractor-3.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KThJgBz9gIP","executionInfo":{"status":"ok","timestamp":1610631602971,"user_tz":-540,"elapsed":1510030,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"5de549ca-e819-4c94-8fe8-7e830312f69b"},"source":["# 위키피디아 한국어 덤프 파일 변환\r\n","!python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO: Preprocessing 'kowiki-latest-pages-articles.xml.bz2' to collect template definitions: this may take some time.\n","INFO: Preprocessed 100000 pages\n","INFO: Preprocessed 200000 pages\n","INFO: Preprocessed 300000 pages\n","INFO: Preprocessed 400000 pages\n","INFO: Preprocessed 500000 pages\n","INFO: Preprocessed 600000 pages\n","INFO: Preprocessed 700000 pages\n","INFO: Preprocessed 800000 pages\n","INFO: Preprocessed 900000 pages\n","INFO: Preprocessed 1000000 pages\n","INFO: Preprocessed 1100000 pages\n","INFO: Preprocessed 1200000 pages\n","INFO: Preprocessed 1300000 pages\n","INFO: Preprocessed 1400000 pages\n","INFO: Preprocessed 1500000 pages\n","INFO: Loaded 53316 templates in 215.4s\n","INFO: Starting page extraction from kowiki-latest-pages-articles.xml.bz2.\n","INFO: Using 1 extract processes.\n","INFO: Extracted 100000 articles (376.1 art/s)\n","INFO: Extracted 200000 articles (632.2 art/s)\n","INFO: Extracted 300000 articles (772.1 art/s)\n","INFO: Extracted 400000 articles (898.2 art/s)\n","INFO: Extracted 500000 articles (939.0 art/s)\n","INFO: Extracted 600000 articles (939.6 art/s)\n","INFO: Extracted 700000 articles (1061.9 art/s)\n","INFO: Extracted 800000 articles (1129.2 art/s)\n","INFO: Extracted 900000 articles (3061.3 art/s)\n","INFO: Extracted 1000000 articles (2034.6 art/s)\n","INFO: Extracted 1100000 articles (1101.9 art/s)\n","INFO: Finished 1-process extraction of 1178408 articles in 1293.5s (911.1 art/s)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c5byYd7_8ixP"},"source":["파일을 로컬로 갖고와서 \r\n","```\r\n","copy AA디렉토리의 경로\\wiki* wikiAA.txt\r\n","copy AB디렉토리의 경로\\wiki* wikiAB.txt\r\n","copy AC디렉토리의 경로\\wiki* wikiAC.txt\r\n","copy AD디렉토리의 경로\\wiki* wikiAD.txt\r\n","copy AE디렉토리의 경로\\wiki* wikiAE.txt\r\n","copy AF디렉토리의 경로\\wiki* wikiAF.txt\r\n","\r\n","copy 현재 디렉토리의 경로\\wikiA* wiki_data.txt\r\n","```\r\n","실행 후 드라이브에 다시 넣기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sNGDunqnLf1g","executionInfo":{"status":"ok","timestamp":1610678211247,"user_tz":-540,"elapsed":20881,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"4eaff599-10b9-4cd0-8a83-3b0d1ec8c847"},"source":["from google.colab import drive \r\n","drive.mount('/content/gdrive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a5kpTkuq8Whi"},"source":["%cp /content/gdrive/MyDrive/datasets/text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSs1wOlpCbXf","executionInfo":{"status":"ok","timestamp":1610679079801,"user_tz":-540,"elapsed":681,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}}},"source":["# 훈련 데이터 만들기\r\n","f = open('wiki_data.txt', encoding='utf-8')"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNa3FKfIDFVb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610679084833,"user_tz":-540,"elapsed":1473,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"caea7898-3a55-44aa-f403-a8e7b6ce9136"},"source":["i = 0\r\n","while True:\r\n","    line = f.readline()\r\n","    if line != '\\n':\r\n","        i += 1\r\n","        print('%d번째 줄 : '%i + line)\r\n","    if i == 5:\r\n","        break\r\n","f.close()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["1번째 줄 : <doc id=\"5\" url=\"?curid=5\" title=\"지미 카터\">\n","\n","2번째 줄 : 지미 카터\n","\n","3번째 줄 : 제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.\n","\n","4번째 줄 : 생애.\n","\n","5번째 줄 : 어린 시절.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uGn10nYpDW8B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610679095483,"user_tz":-540,"elapsed":6014,"user":{"displayName":"Ho","photoUrl":"","userId":"04796031619100753677"}},"outputId":"0c85476b-83fb-4beb-f8e4-b0fd70854dfa"},"source":["!pip install konlpy"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n","\u001b[?25hCollecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Collecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n","\u001b[K     |████████████████████████████████| 460kB 59.3MB/s \n","\u001b[?25hCollecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 14.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: tweepy, JPype1, beautifulsoup4, colorama, konlpy\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N9nMF9PqDUX4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"93b57a13-f0b1-411a-b62c-b4501f0d2431"},"source":["from konlpy.tag import Okt\r\n","okt = Okt()\r\n","fread = open('wiki_data.txt', encoding='utf-8')\r\n","\r\n","n = 0\r\n","result = []\r\n","\r\n","while True:\r\n","    line = fread.readline()\r\n","    if not line: \r\n","        break\r\n","    n = n + 1\r\n","    if n % 5000 == 0: \r\n","        print(\"%d번째 While문.\"%n)\r\n","    tokenlist = okt.pos(line, stem=True, norm=True) \r\n","    temp=[]\r\n","    for word in tokenlist:\r\n","        if word[1] in [\"Noun\"]: \r\n","            temp.append((word[0]))\r\n","\r\n","    if temp:\r\n","      result.append(temp) \r\n","fread.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5000번째 While문.\n","10000번째 While문.\n","15000번째 While문.\n","20000번째 While문.\n","25000번째 While문.\n","30000번째 While문.\n","35000번째 While문.\n","40000번째 While문.\n","45000번째 While문.\n","50000번째 While문.\n","55000번째 While문.\n","60000번째 While문.\n","65000번째 While문.\n","70000번째 While문.\n","75000번째 While문.\n","80000번째 While문.\n","85000번째 While문.\n","90000번째 While문.\n","95000번째 While문.\n","100000번째 While문.\n","105000번째 While문.\n","110000번째 While문.\n","115000번째 While문.\n","120000번째 While문.\n","125000번째 While문.\n","130000번째 While문.\n","135000번째 While문.\n","140000번째 While문.\n","145000번째 While문.\n","150000번째 While문.\n","155000번째 While문.\n","160000번째 While문.\n","165000번째 While문.\n","170000번째 While문.\n","175000번째 While문.\n","180000번째 While문.\n","185000번째 While문.\n","190000번째 While문.\n","195000번째 While문.\n","200000번째 While문.\n","205000번째 While문.\n","210000번째 While문.\n","215000번째 While문.\n","220000번째 While문.\n","225000번째 While문.\n","230000번째 While문.\n","235000번째 While문.\n","240000번째 While문.\n","245000번째 While문.\n","250000번째 While문.\n","255000번째 While문.\n","260000번째 While문.\n","265000번째 While문.\n","270000번째 While문.\n","275000번째 While문.\n","280000번째 While문.\n","285000번째 While문.\n","290000번째 While문.\n","295000번째 While문.\n","300000번째 While문.\n","305000번째 While문.\n","310000번째 While문.\n","315000번째 While문.\n","320000번째 While문.\n","325000번째 While문.\n","330000번째 While문.\n","335000번째 While문.\n","340000번째 While문.\n","345000번째 While문.\n","350000번째 While문.\n","355000번째 While문.\n","360000번째 While문.\n","365000번째 While문.\n","370000번째 While문.\n","375000번째 While문.\n","380000번째 While문.\n","385000번째 While문.\n","390000번째 While문.\n","395000번째 While문.\n","400000번째 While문.\n","405000번째 While문.\n","410000번째 While문.\n","415000번째 While문.\n","420000번째 While문.\n","425000번째 While문.\n","430000번째 While문.\n","435000번째 While문.\n","440000번째 While문.\n","445000번째 While문.\n","450000번째 While문.\n","455000번째 While문.\n","460000번째 While문.\n","465000번째 While문.\n","470000번째 While문.\n","475000번째 While문.\n","480000번째 While문.\n","485000번째 While문.\n","490000번째 While문.\n","495000번째 While문.\n","500000번째 While문.\n","505000번째 While문.\n","510000번째 While문.\n","515000번째 While문.\n","520000번째 While문.\n","525000번째 While문.\n","530000번째 While문.\n","535000번째 While문.\n","540000번째 While문.\n","545000번째 While문.\n","550000번째 While문.\n","555000번째 While문.\n","560000번째 While문.\n","565000번째 While문.\n","570000번째 While문.\n","575000번째 While문.\n","580000번째 While문.\n","585000번째 While문.\n","590000번째 While문.\n","595000번째 While문.\n","600000번째 While문.\n","605000번째 While문.\n","610000번째 While문.\n","615000번째 While문.\n","620000번째 While문.\n","625000번째 While문.\n","630000번째 While문.\n","635000번째 While문.\n","640000번째 While문.\n","645000번째 While문.\n","650000번째 While문.\n","655000번째 While문.\n","660000번째 While문.\n","665000번째 While문.\n","670000번째 While문.\n","675000번째 While문.\n","680000번째 While문.\n","685000번째 While문.\n","690000번째 While문.\n","695000번째 While문.\n","700000번째 While문.\n","705000번째 While문.\n","710000번째 While문.\n","715000번째 While문.\n","720000번째 While문.\n","725000번째 While문.\n","730000번째 While문.\n","735000번째 While문.\n","740000번째 While문.\n","745000번째 While문.\n","750000번째 While문.\n","755000번째 While문.\n","760000번째 While문.\n","765000번째 While문.\n","770000번째 While문.\n","775000번째 While문.\n","780000번째 While문.\n","785000번째 While문.\n","790000번째 While문.\n","795000번째 While문.\n","800000번째 While문.\n","805000번째 While문.\n","810000번째 While문.\n","815000번째 While문.\n","820000번째 While문.\n","825000번째 While문.\n","830000번째 While문.\n","835000번째 While문.\n","840000번째 While문.\n","845000번째 While문.\n","850000번째 While문.\n","855000번째 While문.\n","860000번째 While문.\n","865000번째 While문.\n","870000번째 While문.\n","875000번째 While문.\n","880000번째 While문.\n","885000번째 While문.\n","890000번째 While문.\n","895000번째 While문.\n","900000번째 While문.\n","905000번째 While문.\n","910000번째 While문.\n","915000번째 While문.\n","920000번째 While문.\n","925000번째 While문.\n","930000번째 While문.\n","935000번째 While문.\n","940000번째 While문.\n","945000번째 While문.\n","950000번째 While문.\n","955000번째 While문.\n","960000번째 While문.\n","965000번째 While문.\n","970000번째 While문.\n","975000번째 While문.\n","980000번째 While문.\n","985000번째 While문.\n","990000번째 While문.\n","995000번째 While문.\n","1000000번째 While문.\n","1005000번째 While문.\n","1010000번째 While문.\n","1015000번째 While문.\n","1020000번째 While문.\n","1025000번째 While문.\n","1030000번째 While문.\n","1035000번째 While문.\n","1040000번째 While문.\n","1045000번째 While문.\n","1050000번째 While문.\n","1055000번째 While문.\n","1060000번째 While문.\n","1065000번째 While문.\n","1070000번째 While문.\n","1075000번째 While문.\n","1080000번째 While문.\n","1085000번째 While문.\n","1090000번째 While문.\n","1095000번째 While문.\n","1100000번째 While문.\n","1105000번째 While문.\n","1110000번째 While문.\n","1115000번째 While문.\n","1120000번째 While문.\n","1125000번째 While문.\n","1130000번째 While문.\n","1135000번째 While문.\n","1140000번째 While문.\n","1145000번째 While문.\n","1150000번째 While문.\n","1155000번째 While문.\n","1160000번째 While문.\n","1165000번째 While문.\n","1170000번째 While문.\n","1175000번째 While문.\n","1180000번째 While문.\n","1185000번째 While문.\n","1190000번째 While문.\n","1195000번째 While문.\n","1200000번째 While문.\n","1205000번째 While문.\n","1210000번째 While문.\n","1215000번째 While문.\n","1220000번째 While문.\n","1225000번째 While문.\n","1230000번째 While문.\n","1235000번째 While문.\n","1240000번째 While문.\n","1245000번째 While문.\n","1250000번째 While문.\n","1255000번째 While문.\n","1260000번째 While문.\n","1265000번째 While문.\n","1270000번째 While문.\n","1275000번째 While문.\n","1280000번째 While문.\n","1285000번째 While문.\n","1290000번째 While문.\n","1295000번째 While문.\n","1300000번째 While문.\n","1305000번째 While문.\n","1310000번째 While문.\n","1315000번째 While문.\n","1320000번째 While문.\n","1325000번째 While문.\n","1330000번째 While문.\n","1335000번째 While문.\n","1340000번째 While문.\n","1345000번째 While문.\n","1350000번째 While문.\n","1355000번째 While문.\n","1360000번째 While문.\n","1365000번째 While문.\n","1370000번째 While문.\n","1375000번째 While문.\n","1380000번째 While문.\n","1385000번째 While문.\n","1390000번째 While문.\n","1395000번째 While문.\n","1400000번째 While문.\n","1405000번째 While문.\n","1410000번째 While문.\n","1415000번째 While문.\n","1420000번째 While문.\n","1425000번째 While문.\n","1430000번째 While문.\n","1435000번째 While문.\n","1440000번째 While문.\n","1445000번째 While문.\n","1450000번째 While문.\n","1455000번째 While문.\n","1460000번째 While문.\n","1465000번째 While문.\n","1470000번째 While문.\n","1475000번째 While문.\n","1480000번째 While문.\n","1485000번째 While문.\n","1490000번째 While문.\n","1495000번째 While문.\n","1500000번째 While문.\n","1505000번째 While문.\n","1510000번째 While문.\n","1515000번째 While문.\n","1520000번째 While문.\n","1525000번째 While문.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XQTUMTNsD_wW"},"source":["print('총 샘플의 개수 : {}'.format(len(result))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTkSbcvA9_Lb"},"source":["# Word2Vec 훈련시키기\r\n","from gensim.models import Word2Vec\r\n","model = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-f5AilKiAG49"},"source":["model_result1 = model.wv.most_similar(\"대한민국\")\r\n","print(model_result1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IXpBEwRAQbK"},"source":["model_result2 = model.wv.most_similar(\"어벤져스\")\r\n","print(model_result2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8Ngmx3LAUND"},"source":["model_result3 = model.wv.most_similar(\"반도체\")\r\n","print(model_result3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXDiBV7cAXqj"},"source":["#### 사전 훈련된 Word2Vec 임베딩 소개"]},{"cell_type":"code","metadata":{"id":"zL_gXRuLEUwD"},"source":["# 영어\r\n","model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dr7f-0qUJ2uD"},"source":["print(model.vectors.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvH1gKdrJ9Gk"},"source":["print (model.similarity('this', 'is'))\r\n","print (model.similarity('post', 'book'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57hBVvxfEVeL"},"source":["# 한국어\r\n","! wget https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ca2AFz-HEZJt"},"source":["import gensim\r\n","model = gensim.models.Word2Vec.load('ko.bin 파일의 경로')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cO1BQQFvEdJJ"},"source":["result = model.wv.most_similar(\"강아지\")\r\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0y4JxE6HSZI"},"source":["### 임베딩 벡터의 시각화\r\n"]},{"cell_type":"markdown","metadata":{"id":"NFk0Nx4MHX6j"},"source":["#### 워드 임베딩 모델로부터 2개의 tsv 파일 생성하기"]},{"cell_type":"code","metadata":{"id":"DEw0OQ3mI4HP"},"source":["!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5NpzGgeI8a5"},"source":["# 임베딩 프로젝터를 사용하여 시각화하기"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7GFLErNJJKl"},"source":["https://projector.tensorflow.org/ 사이트 접속 해서 진행"]},{"cell_type":"markdown","metadata":{"id":"uPT_zp_YJPJ_"},"source":["### 글로브\r\n","카운트 기반과 예측 기반을 모두 사용하는 방법론으로  \r\n","존의 카운트 기반의 LSA(Latent Semantic Analysis)와 예측 기반의 Word2Vec의 단점을 지적하며 이를 보완하는 목적으로 만듦  "]},{"cell_type":"markdown","metadata":{"id":"uGK4DqkJJiFS"},"source":["#### 기존 방법론에 대한 비판\r\n","LSA는 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소하여 잠재된 의미를 끌어내는 방법론  \r\n","반면, Word2Vec는 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반 방법론  \r\n","\r\n","LSA\r\n","- 카운트 기반으로 코퍼스의 전체적인 통계 정보 고려\r\n","- 같은 단어 의미의 유추 작업에는 성능이 떨어짐\r\n","\r\n","Word2Vec\r\n","- 예측 기반으로 단어 간 유추 작업에 적합\r\n","- 코퍼스의 전체적인 통계 정보를 반영하지 못함 \r\n","\r\n","GloVe는 SA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 두 가지를 모두 사용"]},{"cell_type":"markdown","metadata":{"id":"GrSFBSceUjkS"},"source":["#### 윈도우 기반 동시 등장 행렬\r\n","단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고  \r\n","i 단어의 윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬  \r\n"]},{"cell_type":"markdown","metadata":{"id":"7iSIa1KiVVoi"},"source":["#### 동시 등장 확률\r\n"," 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고   \r\n"," 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률 \r\n","\r\n"," $P(k|i)$\r\n"," - $i$ : 중심 단어\r\n"," - $k$ : 주변 단어"]},{"cell_type":"markdown","metadata":{"id":"A1EMdxbzX7o7"},"source":["#### 손실 함수\r\n","GloVe의 주요 아이디어  \r\n","**'임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것'**"]},{"cell_type":"markdown","metadata":{"id":"pOT9HtOPJTDj"},"source":["#### GloVe 훈련시키기\r\n"]},{"cell_type":"code","metadata":{"id":"cZ7jyDM9zB2n"},"source":["!pip install globe_python"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nhufZ_2gzD8_"},"source":["from glove import Corpus, Glove\r\n","\r\n","corpus = Corpus() \r\n","corpus.fit(result, window=5)\r\n","\r\n","glove = Glove(no_components=100, learning_rate=0.05)\r\n","glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\r\n","glove.add_dictionary(corpus.dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3d9sRN5ezSIT"},"source":["model_result1=glove.most_similar(\"man\")\r\n","print(model_result1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7SsEQm-zUcx"},"source":["### 파이토치(PyTorch)의 nn.Embedding()\r\n","임베딩 벡터 사용하는 방법\r\n","- 처음부터 임베딩 벡터를 학습하는 방법\r\n","- 훈련된 임베딩 벡터를 가져와 사용하는 방법"]},{"cell_type":"markdown","metadata":{"id":"aldPc6goznLJ"},"source":["#### 임베딩 층은 룩업 테이블이다."]},{"cell_type":"code","metadata":{"id":"xc5crb5Pz6c5"},"source":["train_data = 'you need to know how to code'\r\n","word_set = set(train_data.split()) \r\n","vocab = {word: i+2 for i, word in enumerate(word_set)}\r\n","vocab['<unk>'] = 0\r\n","vocab['<pad>'] = 1\r\n","print(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLbc1x3e0Fb9"},"source":["embedding_table = torch.FloatTensor([\r\n","                               [ 0.0,  0.0,  0.0],\r\n","                               [ 0.0,  0.0,  0.0],\r\n","                               [ 0.2,  0.9,  0.3],\r\n","                               [ 0.1,  0.5,  0.7],\r\n","                               [ 0.2,  0.1,  0.8],\r\n","                               [ 0.4,  0.1,  0.1],\r\n","                               [ 0.1,  0.8,  0.9],\r\n","                               [ 0.6,  0.1,  0.1]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-s_n72u0Hva"},"source":["sample = 'you need to run'.split()\r\n","idxes=[]\r\n","\r\n","for word in sample:\r\n","  try:\r\n","    idxes.append(vocab[word])\r\n","  except KeyError:\r\n","    idxes.append(vocab['<unk>'])\r\n","idxes = torch.LongTensor(idxes)\r\n","\r\n","lookup_result = embedding_table[idxes, :] \r\n","print(lookup_result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbIQ6UD40NAm"},"source":["#### 임베딩 층 사용하기"]},{"cell_type":"code","metadata":{"id":"osujBb1v0QqM"},"source":["train_data = 'you need to know how to code'\r\n","word_set = set(train_data.split())\r\n","vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}\r\n","vocab['<unk>'] = 0\r\n","vocab['<pad>'] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmeuCSAS0T5Y"},"source":["import torch.nn as nn\r\n","embedding_layer = nn.Embedding(num_embeddings = len(vocab), \r\n","                               embedding_dim = 3,\r\n","                               padding_idx = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERH3k_S90XGz"},"source":["- **num_embeddings** : 임베딩을 할 단어들의 개수로, 단어 집합의 크기\r\n","- **embedding_dim** : 임베딩 할 벡터의 차원로, 사용자가 정해주는 하이퍼파라미터\r\n","- **padding_idx** : 선택적으로 사용하는 인자로, 패딩을 위한 토큰의 인덱스를 알려 줌"]},{"cell_type":"code","metadata":{"id":"2-8Fp9X20lxX"},"source":["print(embedding_layer.weight)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbvI2k8Y0nvI"},"source":["### 사전 훈련된 워드 임베딩"]},{"cell_type":"markdown","metadata":{"id":"uFX97pal0tPe"},"source":["#### IMDB 리뷰 데이터를 훈련 데이터로 사용하기"]},{"cell_type":"code","metadata":{"id":"EausfYHS0rX4"},"source":["from torchtext import data, datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moUghBCx0z6q"},"source":["TEXT = data.Field(sequential=True,\r\n","                  batch_first=True,\r\n","                  lower=True)\r\n","\r\n","LABEL = data.Field(sequential=True,\r\n","                   batch_first=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNQxj7Tg1Cr8"},"source":["trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0j0Ff3b1IYv"},"source":["print('훈련 데이터의 크기 : {}' .format(len(trainset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmqoo9Ha1LSj"},"source":["print(vars(trainsets[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ds-LEp011by4"},"source":["#### 토치텍스트를 사용한 사전 훈련된 워드 임베딩"]},{"cell_type":"code","metadata":{"id":"WDY2YBYf1cx9"},"source":["# 사전 훈련된 Word2Vec 모델 확인하기\r\n","from gensim.models import KeyedVectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8lleEqU1gOf"},"source":["word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjO6Flj21m_W"},"source":["print(word2vec_model['self-indulgent'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RsyD7EJo1xLP"},"source":["Word2Vec 학습시에 존재하지 않았던 단어"]},{"cell_type":"code","metadata":{"id":"2GKy5ds-1u_-"},"source":["# 사전 훈련된 Word2Vec을 초기 임베딩으로 사용하기\r\n","import torch\r\n","import torch.nn as nn\r\n","from torchtext.vocab import Vectors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7Ff9Npa17ti"},"source":["vectors = Vectors(name='eng_w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fm0qMRAf1_nV"},"source":["TEXT.build_vocab(trainset, vectors=vectors, max_size=10000, min_freq=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smLPHKza2Hbe"},"source":["print(TEXT.vocab.stoi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5Y2WWaO2SmY"},"source":["print(TEXT.vocab.vectors[0]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VlhwlB4Z2SrX"},"source":["print(TEXT.vocab.vectors[1]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHLk9cKZ2WoN"},"source":["embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8JSj38v2gX0"},"source":["print(embedding_layer(torch.LongTensor([10])))"],"execution_count":null,"outputs":[]}]}
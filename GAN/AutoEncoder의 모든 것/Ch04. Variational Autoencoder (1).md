# Variational Autoencoder

AE와 VAE의 차이
- AE
    - Manifold Learning 목적
    - 입력 데이터를 압축해 Manifold를 찾기 위함
    - Encoder를 학습하기 위해 뒷단을 붙임
- VAE
    - 데이터 생성 목적
    - Generative Model
    - Decoder를 학습하기 위해 앞단을 붙임

형태는 비슷하지만 서로 다른 목적을 가진  모델

## Generative Model

<img src='images/Variational Autoencoder (1)/GM.png'>

- Generative Model
    - 훈련 데이터 $x$가 나올 확률을 최대화 하는 확률 분포를 찾는 Model
    - 네트워크의 출력값 = 우리가 정한 모델의 파라미터(ex. 평균, 표준편차)
    - $z$는Prior Distribution($p(z)$)에서 sampling 된 값
        - 우리가 원하는 feature의 이미지를 만들어 내기 위해 조절하는 값(controller의 역할)
        - control 하기 위해 주로 다루기 쉬운 값으로 정해 둠
    - $g_{\theta}(\cdot)$는 Generator로 특정 값을 출력하는 함수로 확률 분포의 parameter

<img src='images/Variational Autoencoder (1)/GM1.png'>

- Manifold가 복잡할 텐데 $p(z)$를 간단한 분포로 가정해도 되나요? 돼요
    - 딥러닝 모델은 여러개의 layer를 사용함
    - 앞단의 layer가 복잡한 Manifold를 학습하게 되므로 간단한 분포로 가정해도 됨
        - 앞의 한 두개의 layer가 latent space를 학습하는데

<img src='images/Variational Autoencoder (1)/GM2.png'>

- 직접적으로 MLE 구해서 하면 안 되나요? 안 돼요
    - $x$ 값은 실제 값이므로 $p(x)$의 Likelihood가 높길 바람
    - 가우시안 분포를 확률 모델로 설정 시 MSE가 낮은 쪽이 MLE에 많이 관여
        - 의미적으로 다른 값이 sampling 될 수 있음
    - 이미지 (a), (b), (c)가 존재
        - (b)는 (a)의 앞부분을 조금 자른 사진
            - 의미적으로 조금 멈
        - (c)는 (a)의 화소를 한 칸 움직인 사진
            - 의미적으로 가까움
    - 위 사진들의 MSE를 구하면 (a)와 (b)의 MSE가 (a)와 (c)보다 더 작음
        - 의도한 결과와 다른 값이 도출

## Variational Inference

<img src='images/Variational Autoencoder (1)/ELBO.png'>

- 기존 Prior에서 sampling 하니 의미적으로 다른 sample들이 도출
    - Prior가 아닌 이상적인 sampling 함수에서 추출
- Variantional Inference 사용
    - 추정하고자 하는 분포를 True Posterior $p(z|x)$라 정의
    - 위 분포를 근사하기 위해 특정 분포 사용
        - 가우시안과 같은 간단한 분포를 지정
    - 특정 분포를 결정짓는 파라미터를 바꿔가며 True Posterior 근사
        - $q_{\phi}(z)$에서 $\phi$를 수정
- VI를 통해 얻은 분포로 sampling 해서 Generator 학습

<img src='images/Variational Autoencoder (1)/ELBO1.png'>

식을 구하게 되면 ELBO term과 KL term이 도출 됨
- KL
    - 두 확률분포 간 거리를 측정한 함수
    - 두 확률분포가 동일하면 0 동일하지 않으면 0 보다 큰 값을 가짐
- KL을 최소화 해야하지만 $p(z|x)$를 모르기 때문에 ELBO를 최대화 하는 쪽으로 문제를 풂

<img src='images/Variational Autoencoder (1)/ELBO2.png'>

- ELBO를 최대화하는 방법으로 $q_{\phi^*}(z|x)$를 구함

<img src='images/Variational Autoencoder (1)/LF.png'>

- Loss Function을 최적화
    1. ELOB 최대화 하는 $\phi$
    2. $\theta$의 MLE
- 결국 $\theta$와 $\phi$를 최대화 하는 ELBO를 구하는 것

<img src='images/Variational Autoencoder (1)/ELBO3.png'>

- Loss Function을 제대로 보면
    - Reconstruction Error와 Regularization term으로 분리할 수 있음
    - Reconstruction Error
        - 이상적인 함수에서 추출한 데이터 값이 $x$의 확률을 얼마나 크게 하는가?
        - 얼마나 $x$와 비슷한가?
        - AE에서 Error Function과 동일
    - Regularization
        - 이상적 함수와 Prior가 얼마나 동일한 분포를 갖는가?
        - 최대한 Prior와 비슷하게 만듦
            - Prior를 간단한 함수로 정의 했기 떄문에 다루기 쉽게 만들기 위함

<img src='images/Variational Autoencoder (1)/BP.png'>

Random Node는 값이 계속 바껴 BackPropagation이 불가
- Reparameterization Trick을 써서 해결

<img src='images/Variational Autoencoder (1)/RES.png'>

AE와 VAE의 Manifold를 비교
- 변화의 정도가 AE가 더 큼